---
title: "Mushroom Classification"
author: "Group 8"
date: "Dec 19, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


- Install necessary packages

```{r packages}
install.packages(c("tidyverse", "ggplot2", "caTools", "mice", "caret", "MASS", "rpart", "rpart.plot", "leaps", "corrplot", "dummies", "klaR", "pROC", "randomForest"))
library(tidyverse)
library(ggplot2)# For making data visualization
library(caTools) # For smaple split
library(mice) # For impute missing data
library(caret)# For train and Cross validation 
library(MASS) # For LDA model
library(rpart) # For decision tree model
library(rpart.plot) # For decision tree map
library(leaps) # For subset regression
library(corrplot) # For correlation function
library(dummies) # For one-hot encoding
library(klaR) # For k-mode function/LDA partition map 
library(pROC) # For ROC curve 
library(randomForest) # For randomForest
```

- Import dataset

```{r data}
getwd()
setwd("/Users/Sophie/Downloads/BA with R") # This part changes regarding your OS
mushrooms <- read.csv('mushrooms.csv')
mushrooms <- read.csv('mushrooms.csv')

```


- EDA
```{r EDA}
summary(mushrooms)
str(mushrooms)
levels(mushrooms$stalk.root)
```

- Cleaning Data
  From EDA above could find 'veil.type' has only 1 class; it can be removed.
  Stalk.root has '?' class. This is a missing value. To prevent from recognizing '?' as a class, code it to NA. 

```{r more EDA}
mushrooms$veil.type<-NULL

mushrooms$stalk.root <- as.character(mushrooms$stalk.root)
mushrooms$stalk.root[mushrooms$stalk.root == "?"] <- NA
mushrooms$stalk.root <- as.factor(mushrooms$stalk.root)

```

According to the data dictionary, stalk.root includes two more classes: 'u' and 'z'.
We have to hink about whether adding these two classes or not.

```{r}
#levels(mushrooms$stalk.root) <- c(levels(mushrooms$stalk.root), "u", "z")
levels(mushrooms$stalk.root)
summary(mushrooms$stalk.root)
```


- Dealing with Missing Values
  We used 'mice' package: Polyreg and LDA

```{r Missing Values: polyreg or lda}

# Using polyreg method:
polyreg_mushrooms <- mice(mushrooms, m=5, seed=1, method = 'polyreg', print = FALSE)
summary(polyreg_mushrooms)
densityplot(polyreg_mushrooms, ~stalk.root)

# Using lda method:
lda_mushrooms <- mice(mushrooms, m=5, seed=1, method = 'lda', print = FALSE)
summary(lda_mushrooms)
densityplot(lda_mushrooms, ~stalk.root)

```


-  There is a huge difference between Polyreg and LDA as an Imputation method.
  (Polyreg VS. LDA)
  According to density plot, LDA seems to be more reasonable.

```{r}

data.lda_mushrooms <- complete(lda_mushrooms)
levels(data.lda_mushrooms$stalk.root)
summary(data.lda_mushrooms$stalk.root)

```


- Label Encoding
  To deal with categotical variables, we changed categorical variables to numeric variables
  We used 'label encoding.' About 'label encoding,' please refer to https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/ 
  However, there are some drawbacks in this method. 
  There are other methods to deal with categorical varialbes such as one hot encoding and using a dummy varialbe. 
  If we apply one-hot endoing to this dataset, the number of total independatant variables will be 115.
  Having too many variables can cause some drawbacks, such as multicollinearity, which was beyond our coverage.
  So, to make it simple, we applied 'label encoding' method.

```{r Label encoding}

DF <- as.data.frame(unclass(data.lda_mushrooms))
convert<-sapply(DF,is.factor)
d1<-sapply(DF[,convert],unclass)    
data.lda_mushrooms<-cbind(d1[,!convert],d1)        
data.lda_mushrooms <- data.frame(data.lda_mushrooms)
head(data.lda_mushrooms)
str(data.lda_mushrooms)
data.lda_mushrooms <- data.frame(data.lda_mushrooms)
str(data.lda_mushrooms)
data.lda_mushrooms$class<-ifelse(data.lda_mushrooms$class=='2',0,1)
data.lda_mushrooms[,-1] = scale(data.lda_mushrooms[,-1], center = T, scale = T) # scale the data

# The code for one-hot encoding (just for reference)
data.lda_mushrooms.onehot <- complete(lda_mushrooms)
clas1 <- data.lda_mushrooms.onehot$class
data.lda_mushrooms.onehot$class <- NULL
data.lda_mushrooms.onehot <- dummy.data.frame(data.lda_mushrooms.onehot, names=colnames(data.lda_mushrooms.onehot), sep="_")
data.lda_mushrooms.onehot$class <- clas1
str(data.lda_mushrooms.onehot)

# The statistical relationship between variables
a <- cor(data.lda_mushrooms)
corrplot(a, method = "color")

#a1 <- cor(data.lda_mushrooms.onehot)
#corrplot(a1, method = "color")

# Chanbe the type of our target variable to 'factor'
data.lda_mushrooms$class[data.lda_mushrooms$class==0] <- 'p'
data.lda_mushrooms$class[data.lda_mushrooms$class==1] <- 'e'
data.lda_mushrooms$class <- as.factor(data.lda_mushrooms$class)
str(data.lda_mushrooms)

# The relationship with 'class' variable and other variables.
lda_data <- data.lda_mushrooms[,2:22]
lda_class <- data.lda_mushrooms[,1]
scales <- list(x=list(relation="free"),y=list(relation="free"), cex=0.6)
featurePlot(x=lda_data, y=lda_class, plot="density",scales=scales,
            layout = c(4,6), auto.key = list(columns = 2), pch = "|")
```


- Variable Selection
  1. Best subset selection
  2. Forward and backward selection

```{r Variable selection}
# 1. Best subset selection
regfit.full=regsubsets(data.lda_mushrooms$class~., data.lda_mushrooms)
summary(regfit.full)
regfit.max = regsubsets(data.lda_mushrooms$class~., data.lda_mushrooms, nvmax=21)
regsummary = summary(regfit.max)
regsummary$rsq
par(mfrow=c(2,2))
plot(regsummary$rss, xlab="Number of variables", ylab="RSS", type='l')
plot(regsummary$adjr2,xlab="Number of variables", ylab="Adjusted Rsq", type='l')
which.max(regsummary$adjr2)
points(19,regsummary$adjr2[19],col='red')

# 2. Forward and backward selection
regfit.fwd=regsubsets(class~., data=data.lda_mushrooms, nvmax=21, method="forward")
summary(regfit.fwd)
regfit.bwd=regsubsets(class~., data=data.lda_mushrooms, nvmax=21, method="backward")
summary(regfit.bwd)

```


```{r result of best subset selection}

# Using the best subset selection 
# According to the best subset selection, 19 variables are important without stalk.color.below.ring and cap.color
data.lda_mushrooms1 <- data.lda_mushrooms
data.lda_mushrooms1$stalk.color.below.ring <- NULL
data.lda_mushrooms1$cap.color <- NULL

#data.lda_mushrooms1 <- sapply(data.lda_mushrooms1, function(x) as.factor(as.numeric(x)))
#data.lda_mushrooms1 <- data.frame(data.lda_mushrooms1)
#data.lda_mushrooms1$class<-ifelse(data.lda_mushrooms1$class=='2',0,1)
#data.lda_mushrooms1$class[data.lda_mushrooms1$class==0] <- 'p'
#data.lda_mushrooms1$class[data.lda_mushrooms1$class==1] <- 'e'
#data.lda_mushrooms1$class <- as.factor(data.lda_mushrooms1$class)

str(data.lda_mushrooms1)
```


- Creating our Train set and Test set
  Cross-validation

```{r creating our train set and test set}

# Dividing our dataset into a training set and a test set (Validation set)
set.seed(1024) 
sample = sample.split(data.lda_mushrooms1$class, SplitRatio = .7)
lx_train = subset(data.lda_mushrooms1, sample == TRUE)
lx_test = subset(data.lda_mushrooms1, sample == FALSE)
ly_train<-lx_train$class
ly_test <-lx_test$class
lx_train$class<-NULL
lx_test$class<-NULL

# Cross-validation (k-fold method using caret)
# Lda dataset
l.cv10fold <- createMultiFolds(ly_train, k=10)
lda.train.control <- trainControl(method = 'repeatedcv', number = 10, index=l.cv10fold)

```


- Set Our Models, Predict, and Evaluation
  Logistic Regression & LDA for our dataset transformed into numeric values
  Decision Tree, Random Forest for our dataset of categorical values
  K-modes

 1. Logistic Regression
```{r Logistic regression}

# ------------Model 1. Logistic Regression--------------
# names(getModelInfo())
# lda dataset - using numeric dataset

# Set a model using our training set
l.logistic <- train(x=lx_train,y=ly_train,method="glm", 
                    family="binomial", trControl=lda.train.control) # training set
l.logistic
summary(l.logistic)

# Predict using our test set and check the preidction 
ly.logistic.predicted<-predict(l.logistic,lx_test)
varImp(l.logistic)
df1 <- data.frame(test=ly_test,Pred=ly.logistic.predicted)
confusionMatrix(table(df1$test,df1$Pred))
plot(varImp(l.logistic),main="Logistic regression - Variable Importance Plot")

```

 2. LDA
```{r LDA}
# ------------Model 2. LDA--------------
# Lda dataset - using numeric dataset

# Set a model using our traing set
l.lda <- train(x=lx_train,y=ly_train,method="lda", trControl=lda.train.control)
l.lda
varImp(l.lda)

# Predict using our test set and check the preidction
ly.lda.predicted<-predict(l.lda,lx_test)
df2 <- data.frame(test=ly_test,Pred=ly.lda.predicted)
confusionMatrix(table(df2$test,df2$Pred))
plot(varImp(l.lda),main="LDA - Variable Importance Plot")

```

- reset our values to categorical values
```{r analysis on categorical variables as it is}
# Changing all the categorical independent variables to numerical variables has intrisic drawbacks that deteriorate the model perforamance. 
# Therefore, we tried to create a better model while maintaining our variables as categorical.

# Creating a new validation set for tree method 
cate_variable <- complete(lda_mushrooms)
set.seed(1024) 
sample = sample.split(cate_variable$class, SplitRatio = .7)
cx_train = subset(cate_variable, sample == TRUE)
cx_test = subset(cate_variable, sample == FALSE)
cy_train<-cx_train$class
cy_test <-cx_test$class
cx_train$class<-NULL
cx_test$class<-NULL

# Cross-validation (k-fold method using 'caret' package)
c.cv10fold <- createMultiFolds(cy_train, k=10)
c.train.control <- trainControl(method = 'repeatedcv', number = 10, index=c.cv10fold)

```

 3. Decision Tree

```{r Decision tree}
# ------------Model 3. Decision Tree----------------
rpart.grid <- expand.grid(.cp=0)
c.rpart <- train(x=cx_train,y=cy_train,method="rpart",tuneGrid=rpart.grid, trControl=c.train.control)
c.rpart
varImp(c.rpart)
cy.rpart.predicted<-predict(c.rpart,cx_test)
df3 <- data.frame(test=cy_test,Pred=cy.rpart.predicted)
confusionMatrix(table(df3$test,df3$Pred))
plot(varImp(c.rpart),main="Decision Tree - Variable Importance Plot")
rpart.plot(c.rpart$finalModel, cex=0.6)
```

 4. Random Forest

```{r Random forest}
# ------------Model 4. Random Forest----------------
c.rf <- train(x=cx_train,y=cy_train,method="rf", trControl=c.train.control)
c.rf
varImp(c.rf)
cy.rf.predicted<-predict(c.rf,cx_test)
df4 <- data.frame(test=cy_test,Pred=cy.rf.predicted)
confusionMatrix(table(df4$test,df4$Pred))
plot(varImp(c.rf),main="Random Forest - Variable Importance Plot")
```

 5. K-modes

```{r K-modes}
# ---------------Model 5. K-modes (extra) - unspervised learning ----------------
# In addition to the above methods, we tried 'K-modes (unspervised learning)' to see how well clustering works in this case, and which category in variables are significant. However, the result were not meaningful enough to use this method for our case.

set.seed(256)
colnames(mushrooms)

# We chose 3 most important variables from above
m_kmode <- kmodes(data = mushrooms[,c(6,10,20)], mode = 2, iter.max = 10) 

m_kmode

table(factor(m_kmode$cluster), factor(mushrooms$class))
m_kmode.pred <- m_kmode$cluster
m_kmode.pred <- ifelse(m_kmode.pred == 1, "p", "e")
m_kmode.pred <- as.factor(m_kmode.pred)
summary(m_kmode.pred)
confusionMatrix(m_kmode.pred, factor(mushrooms$class))

# K-modes clustering only gives us 61.94% of accuracy, although we chose the three most important variables. Also, if we change the lable with the opposite way, the accuracy will be lower. 
# This is significantly low accuracy, considering that we are dealing with the matter of life and death depending on the decision of this method.
# Therfore, we cannot choose this model for our analytics.
```
